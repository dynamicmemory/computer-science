decathlon <- subset(data, sport=="Decathlon")
heptathlon <- subset(data, sport=="Heptathlon")
# Create "eas'ier' to use" variables
d_javelin <- decathlon$javelin
h_javelin <- heptathlon$javelin
# Grab some sample stats for our two groups
summary(d_javelin)
summary(h_javelin)
# obtain the sacred standard devs
sd(d_javelin)
sd(h_javelin)
# checking the heptathlon data to inspect an outlier we observed in the boxplot
hist(h_javelin, breaks = 10, xlab="Javelin scores (Heptathlon)",
main ="", col="pink")
# decathlon scores just to be sure, using a finer bin amount for more definition
hist(d_javelin, breaks = 20, xlab="Javelin scores (Decathlon)",
main ="", col="lavender")
# The t-test
t.test(javelin~sport)
# using our defined heptathlon grouping from above we will make two new groups
hep_long = heptathlon$long_jump
hep_country = heptathlon$country
# Create a boxplot to see what we are working with here
boxplot(hep_long~hep_country, ylab="Long Jump Score (meters)",
xlab="Athletes Country",
col=c("lightblue", "salmon", "lightgreen", "gold"))
# lets see how many instances of athletes we have per country
table(hep_country)
# and now let's get a little convoluted by calling sapply to create a matrix
# of summary stats for long jump scores split by country... fancy i know.
sapply(split(hep_long, hep_country), summary)
# Because im feeling lazy, we are going to do the same for the IQR's and std's
sapply(split(hep_long, hep_country), IQR)
sapply(split(hep_long, hep_country), sd)
# Creating a factor out of our countries to use in a qqplot
countries = factor(hep_country)
# Using a qqplot to double check normality of our countries scores
qqnorm(hep_long, col=countries, pch=19, cex = 0.75, main = "")
legend(-2.5, 6.5, legend = c("CAN","CZH","ITA","RUS"),
col = c("black", "salmon", "green", "skyblue"), pch = 19, bty = "n")
# Running the anova test
# Create our linear model
long_jump.aov <- lm(hep_long~hep_country)
# obtain the output
anova(long_jump.aov)
# Our p value shows there is differences, so now we want to find where they are
pairwise.t.test(hep_long, hep_country, p.adjust.method = "bonf")
# Again we will start by making variables for easier access to our information
dec_age = decathlon$age
dec_hurdle = decathlon$hurdles
# Since our ages are in days from birth to competition start, we want to change
# that to just "athletes age in years", so below we will adjust our decathlon
# ages by dividing each by 365 to get the age equiv of each athlete.
# We technically don't need to do this, but it will make interpreting our
#results a whole lot less convoluted talking about days old vs years, which
# is far more intuitive.
dec_age <- dec_age / 365
# Now we plot the age of our athlete vs there hurdle performance
plot(dec_age, dec_hurdle, ylab = "110m Hurdle time (seconds)",
xlab = "Athletes age (years)", pch = 15, col = "violet", cex = 0.75)
# We had trouble analysis the strength of our relationship so below we will
# find the correlation coef between our variables
cor(dec_age, dec_hurdle)
# Creating our linear model and checking for linearity for our test conditions
lin_reg = lm(dec_hurdle~dec_age)
abline(lin_reg)
# Now we plot the age of our athlete vs there hurdle performance
plot(dec_age, dec_hurdle, ylab = "110m Hurdle time (seconds)",
xlab = "Athletes age (years)", pch = 15, col = "violet", cex = 0.75)
# Creating our linear model and checking for linearity for our test conditions
lin_reg = lm(dec_hurdle~dec_age)
abline(lin_reg)
# Creating our linear model and checking for linearity for our test conditions
lin_reg = lm(dec_hurdle~dec_age)
abline(lin_reg)
# Get our plots to check for vairance and distribution of the residuals in our
# data
plot(lin_reg, which=1)
plot(lin_reg, which=2)
data <- read.csv("../workshop-datasets/Finches_Dataset.csv")
attach(data)
boxplot(Sex)
boxplot(Sex~data)
boxplot(data~Sex)
boxplot(Beak_Length~Sex)
reg_lin <- lm(Weight~Tarsus)
plot(Weight, Tarsus)
abline(reg_lin)
reg_lin <- lm(Tarsus~Weight)
plot(Weight, Tarsus)
abline(reg_lin)
boxplot(Beak_Length~Sex, xlab = "Birds private parts", ylab = "how long dey beak?",
colors = c("red", "blue", "green"))
boxplot(Beak_Length~Sex, xlab = "Birds private parts", ylab = "how long dey beak?",
col = c("red", "blue", "green"))
reg_lin <- lm(Tarsus~Weight)
plot(Weight, Tarsus, pch = 17, col = "lightgreen", xlab = "How fat dey",
ylab = "yo dey length doe?")
abline(reg_lin, col = "pink")
boxplot(Beak_Depth~Survived)
summary(Beak_Depth~Survived)
summary(Beak_Depth)
qqnorm(Beak_Depth~Survived)
qqnorm(Beak_Depth~Survived)
qqplot(Beak_Depth~Survived)
qqnorm(Beak_Depth)
qqnorm(Beak_Depth, c = survived)
survived = factor(Survived)
qqnorm(Beak_Depth, c = survived)
qqnorm(Beak_Depth, col = survived)
t.test(Beak_depth~Survived)
t.test(Beak_Depth~Survived)
t.test(Beak_Depth~Survived, alternate = "less")
t.test(Beak_Depth~Survived, alternative = "less")
abline(reg_lin, col = "pink")
reg_lin <- lm(Tarsus~Weight)
plot(Weight, Tarsus, pch = 17, col = "lightgreen", xlab = "How fat dey",
ylab = "yo dey length doe?")
abline(reg_lin, col = "pink")
# Independence - all birds are birds bitch
# Normal residuals
plot(reg_lin, which =1)
# Independence - all birds are birds bitch
# Normal residuals
plot(reg_lin, which = 0)
# Independence - all birds are birds bitch
# Normal residuals
plot(reg_lin, which = 2)
hist(reg_lin)
shapiro.test(reg_lin)
# constant variance
plot(reg_lin, which=1)
summary(reg_lin)
confint(reg_lin)
ball throwing test?
attach(dataÂ«)
attach(data)
data
detach(data)
detach(data)
####################################
Question: Do players from different positions perform differently on the medicine
ball throwing test?
attach(data)
data
ball throwing test?
attach(rug)
rug <- read.csv("../workshop-datasets/rugby_fitness.csv")
attach(rug)
rug
boxplot(Power_pass~position)
boxplot(Power_pass~Position)
# Independence - we assume indepenedence as each memeber did the test once
# Normal distribution
qqnorm(Power_pass~Postion)
# Independence - we assume indepenedence as each memeber did the test once
# Normal distribution
qqplot(Power_pass~Postion)
# Independence - we assume indepenedence as each memeber did the test once
# Normal distribution
qqplot(Power_pass)
# Independence - we assume indepenedence as each memeber did the test once
# Normal distribution
qqnorm(Power_pass)
# we observe a faily normal distro in that qq plot right there aye
# Constant variance -
summary(Power_pass~Position)
# we observe a faily normal distro in that qq plot right there aye
# Constant variance -
summary(Power_pass)
# we observe a faily normal distro in that qq plot right there aye
# Constant variance -
summary(Position~Power_pass)
# we observe a faily normal distro in that qq plot right there aye
# Constant variance -
sapply(split(Power_pass, Position), sd)
0.653 / 0.78
0.78 / 0.653
0.78 / 0.653 < 2 ? TRUE : FALSE
(0.78 / 0.653 < 2) ? TRUE : FALSE
Power_pass.aov <- lm(Power_pass~Position)
anova(Power_pass.aov)
pairwise.t.test(Power_pass, Position, p.adjust.method = "bonf")
vals = c(44.8, 26.3, 13.6, 48.5, 2.3, 3.2)
var(vals)
z = (105 - 100) / 5.2
pnorm(z)
pbinom(8, 10, 0.52)
dbinom(8, 10, 0.52)
# pbinomn for atleast
1 - pbinom(8, 10, 0.52)
1 - pbinom(9, 114, 0.04)
t = c(15, 27, 18, 22, 21, 28)
mean(c)
mean(t)
sample = mean(t)
x_hat = mean(t)
z = (x_hat - 25) / 0.05
z
pnorm(z)
qnorm(z)
z = (x_hat - 25) / 0.05
z
pnorm(z)
pnorm(-1)
pnorm(-2)
pnorm(z)
t = c(44.85, 49.13, 28.36, 25.77, 41.12, 39.3, 48.14, 28.38)
std = sd(t)
std
error = 1.96 * std
error
x_hat = mean(t)
x_hat + error
x_hat - error
t = c(44.85, 49.13, 28.36, 25.77, 41.12, 39.3, 48.14, 28.38)
std = sd(t)
error = 1.96 * std
error
x_hat = mean(t)
x_hat + error
x_hat - error
t = c(2, 1, 4, 15, 1, 12, 2)
x_hat = mean(t)
z = (x_hat - 8) / 1
pnorm(z)
z
x_hat = 51.05
std = 2.1
mu = 45.7
z = (x_hat - mu) / std
z
# dbinom for exactly
dbinom(8, 10, 0.53)
# pbinomn for atleast
1 - pbinom(8, 10, 0.53)
# pbinomn for atleast
1 - pbinom(8, 10, 0.53)
# pbinomn for atleast
pbinom(8, 10, 0.53)
# pbinomn for atleast
pbinom(8, 10, 0.53, FALSE)
t = c(15, 27, 18, 22, 21, 28)
x_hat = mean(t)
z = (x_hat - 25) / 0.05
z
pnorm(z)
qnorm(-1.54)
qnorm(z)
z - (105 - 100) / 5.2
pnorm(z)
z = (105 - 100) / 5.2
pnorm(z)
z = (105 - 100) / 5.2
pnorm(z)
success_rate = 0.6
aatempts = 13
goals = 8
pbinom(goals, aatempts, succeess_rate, FALSE)
pbinom(goals, aatempts, success_rate, FALSE)
dbinom(goals, aatempts, success_rate)
pbionom(10, 111, 0.06)
pbionom(10, 111, 0.06)
pbinom(10, 111, 0.06)
x_hat = mean(22, 27)
x_hat = mean(c(22,27,26,26,22,27))
vals = c(22,27,26,26,22,27)
x_hat = mean(vals)
z = (x_hat - 25) / 0.05
z
41/42
size = 51 + 4 + 50 + 51
likes = 51 / size
likes = 101 / size
home = 55 / size
overlap = 51 / size
answer = likes + home - overlap
answer
47 / (46 + 47 + 59)
vals = c(49.8, 38.1, 41.22, 28.83, 39.4, 40.92, 38.44, 25.49)
p = c(49.8, 38.1, 41.22, 28.83, 39.4, 40.92, 38.44, 25.49)
se = (p*(1-p)/8)**0.5
vals = c(49.8, 38.1, 41.22, 28.83, 39.4, 40.92, 38.44, 25.49)
p = mean(vals)
se = (p*(1-p)/8)**0.5
vals = c(49.8, 38.1, 41.22, 28.83, 39.4, 40.92, 38.44, 25.49)
p = mean(vals)
se = (p*(1-p)/8)**0.5
se
(p*(1-p)/8)**0.5
p*(1-p)/8)
p*(1-p)/8
p = p*(1-p)/8
p = p**0.5
p = p*0.5
p = p^0.5
p = p^0.5
p = p^0.5
p = p ^ 2
p = (p*(1-p)/8)^0.5
p = (p*(1-p)/8)^0.5
p
p = (p*(1-p)/8)**0.5
p
p = sqrt(p*(1-p)/8)
p
p2 = 1 - p
p3 = p2 * p
vals = c(49.8, 38.1, 41.22, 28.83, 39.4, 40.92, 38.44, 25.49)
p = mean(vals)
p2 = 1 - p
p3 = p2 * p
p4 = p3 / 8
o5 = sqrt(p4)
p5 = sqrt(p4)
p5 = sqrt(abs(p4)
p5 = sqrt(abs(p4))
p = mean(vals)
p2 = 1 - p
p3 = p2 * p
p4 = p3 / 8
p5 = sqrt(abs(p4))
upper = p + 1.96 * p5
lower = p - 1.96 * p5
vals = c(11,8,15,10,13,2,14)
z = x_hat - 8 / 1
std = sd(vals)
z = x_hat - 8 / std
z = (x_hat - 8) / std
x_hat = 54.64
mu = 49.58
std = 1.26
z = (x_hat - mu) / std
vals = c(49.8, 38.1, 41.22, 28.83, 39.4, 40.92, 38.44, 25.49)
x_hat = mean(vals)
std = sd(x_hat)
std = sd(vals)
se = std / (8)**0.5
upper = x_hat + 1.96 * se
lower = x_hat - 1.96 * se
se = std / (7)**0.5
z = (x_hat - 8) / se
z = (x_hat - 8) / std
z = (x_hat - 8) / se
vals = c(11,8,15,10,13,2,14)
x_hat = mean(vals)
std = sd(vals)
se = std / (7)**0.5
z = (x_hat - 8) / se
z = (105 - 100) / 5.2
pnorm(z)
vals = c(22,28,29,29,15,19)
x_hat = mean(vals)
z = (x_hat - 25) / 0.05
z
vals = c(22,28,29,29,15,19)
x_hat = mean(vals)
z = (x_hat - 25) / 0.05
z
x_hat
pnorm(z)
vals = c(47.67,48.53, 40.75, 26.85, 33.57, 27.74, 49.42, 31.72)
x_hat = mean(vals)
s = sd(vals)
se = s / 8**9.5
upper = x_hat + 1.96 * se
lower = x_hat + 1.96 * se
lower = x_hat - 1.96 * se
upper = x_hat + 1.96 * se
lower = x_hat - 1.96 * se
se = s / 8**9.5
upper = x_hat + 1.96 * se
lower = x_hat - 1.96 * se
vals = c(47.67,48.53, 40.75, 26.85, 33.57, 27.74, 49.42, 31.72)
x_hat = mean(vals)
s = sd(vals)
se = s / 8**9.5
upper = x_hat + 1.96 * se
lower = x_hat - 1.96 * se
vals = c(47.67,48.53, 40.75, 26.85, 33.57, 27.74, 49.42, 31.72)
x_hat = mean(vals)
s = sd(vals)
se = s / 8**9.5
upper = x_hat + 1.96 * se
lower = x_hat - 1.96 * se
lower = x_hat * 1.96 * se
lower = x_hat - 1.96 * se
upper = x_hat - 1.96 * se
upper = x_hat + (1.96 * se)
lower = x_hat - (1.96 * se)
upper = 38.28125004 + 1.96 * 2.49977
upper = x_hat + (1.96 * se)
lower = x_hat - (1.96 * se)
x_hat + (1.96 * se)
x_hat - (1.96 * se)
x_hat
me = 1.96 * se
se = s / 8**0.5
upper = x_hat + me
lower = x_hat - (1.96 * se)
me = 1.96 * se
upper = x_hat + me
lower = x_hat - (1.96 * se)
vals = c(7,6,14,6,1,11,9)
x_hat = mean(vals)
s = sd(vals)
se s / 8**0.5
se = s / 8**0.5
z = (x_hat - 8) / se
se = s / **0.5
se = s / 7**0.5
z = (x_hat - 8) / se
z = (50.36 - 45.05) / 4.79
vals = c(22,28,29,29,15,19)
x_hat = mean(vals)
x_hat
s = sd(vals)
se = s / 6**0.5
z = (x_hat - 25) / se
z
total = 42 + 4 + 52 + 60
likes = (42 + 52) / total
cond = 52 / total
first = 4.65**2 / 9
second = 6.6**2 / 9
se = (first + second)**0.5
t_star = (-16.76 - -14.39) / se
se = 5.97 / 18**0.5
-4.74 / se
first = 4.65**2 / 9
second = 6.6**2 / 9
se = (first + second)**0.5
t_star = (-16.76 - -14.39) / se
s1 = 0.314**2 / 50
s2 = 0.322**2 / 50
s3 = s1 + s3
s4 = s3**0.5
s3 = s1 + s2
s4 = s3**0.5
z = (2.974 - 2.77) / s4
g1 = c(86, 51)
g2 = c(52, 18)
choice = rbind(g1, g2)
df = (r -1) * (2 - 1)
df = (2 -1) * (2 - 1)
a = (86 - 91)**2 / 91
b = (51 - 46)**2 / 46
c = (52 - 47)**2 / 47
d = (18 - 23)**2 / 23
t = a + b + c + d
pchisq(, t, df, lower.tail = False)
pchisq(t, df, lower.tail = False)
pchisq(t, df, lower.tail = FALSE)
pnorm(-1.82)
vals = c(43,	34.8,	21.2,	36	,42.9,	36.3)
var(vals)
vals = c(19,	21,	18,	21,	22,	15)
mean(vals)
t = (19.33 - 25) / (0.05/6**0.5)
t
t = (19.33 - 25) / 0.05
t
t = (19.33 - 25) / 1
t
std = s(vals)
std = sd(vals)
se = std/8**0.5
se
x_hat = mean(vals)
upper = x_hat + se * 1.96
lower = x_hat - se * 1.96
t = (19.33 - 25) / (1/6**0.5)
t
vals = c(1,	10,	4,	2,	15,	9,	14)
x_hat = mean(vals)
se = sd(vals) / 7**0.5
t = (x_hat - 8) / se
std = sd(vals)
vals = c(19,	21,	18,	21,	22,	15)
mean(vals)
std = sd(vals)
t = (19.33 - 25) / (std/6**0.5)
t
vals = c(44.21,	26.53,	20.77,	44.4,	48.64,	32.5,	36.67,	44.64)
std = sd(vals)
x_hat = mean(vals)
se = std/8**0.5
se
upper = x_hat + se * 1.96
lower = x_hat - se * 1.96
s1 = 6.57**2 / 20
s2 = 5.54 / 10
s3 = s1 + s2
s1 = 6.57**2 / 20
s2 = 5.54**2 / 10
s3 = s1 + s2
se = s3**0.5
z = (7.65 - 12.41) / se
pbinom(1, 20, 0.2, FALSE)
#Q12
s1 = 6.57**2 / 20
white_expected = 0.46 * 37
non-white_expected = 0.54 * 37
non_white_expected = 0.54 * 37
race = c(31, 6)
race = c(31, 6)
expect = c(0.46, 0.54)
demographic = c(0.46, 0.54)
race = c(31, 6)
demographic = c(0.46, 0.54)
Chit = chisq.test(race, p=demographic)
Chit
